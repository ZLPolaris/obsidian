# 概论
统计学习（statistical learning）是关于计算机基于数据构建概论统计模型并运用模型对数据进行预测域分析的一门学科。

统计学习就是计算机系统通过运用数据提升系统性能的机器学习。
## 步骤

![[Pasted image 20231216143358.png]]

## 分类

![[Pasted image 20231216143445.png]]
## 监督学习
监督学习（Supervised Learning）是指从标注数据中学习预测模型的机器学习问题，其本质是学习输入到输出的映射的统计规律。
### 相关概念
输入空间（input space ）:输入的所可能取值的集合
实例（instance）：每一个具体的输入。通常由特征向量（featur vector）表示
特征空间（feature space）:特征向量存在的空间
输出空间（output space）:输出的所有可能取值的集合

回归问题：输入变量和输出变量均为连续变量的预测问题
分类问题：输出变量为有限个离散变量的预测问题
标注问题：输入变量和输出变量均为变量序列的预测问题

假设空间（hypothesis space）:所有这些可能模型的集合。
符号：
![[Pasted image 20231216144508.png]]


模型：
选择可能性最大的y
![[Pasted image 20231216144930.png]]


## 无监督学习
无监督学习（unsupervised learning）是指从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中的统计规律或潜在结构


![[Pasted image 20231216145224.png]]
## 强化学习
强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为决策的机器学习问题。
## 三要素
### 监督学习
模型 + 策略 + 算法
#### 模型
在监督学习中，模型就是所要学习的决策函数或者概率函数。
模型的假设空间就是所有可能得条件概率分布或决策函数分布。
决策函数：
![[Pasted image 20231216150550.png]]
概率函数：
![[Pasted image 20231216150620.png]]

#### 策略
策略是选择最优模型的学习准则，目标是从假设空间中选取最优模型。

损失函数：度量模型一次预测的好坏。
风险函数：度量平均意义下模型预测的好坏。
经验风险：模型关于训练集和平均损失。

![[Pasted image 20231216151652.png]]
四种常见的损失函数：
![[Pasted image 20231216151724.png]]

经验风险最小化：
结构风险最小化：
![[Pasted image 20231216151932.png]]
最终：策略就是选取一个目标函数（和损失函数先关），让目标函数最小化。
#### 算法
具体计算方法。

### 无监督学习
![[Pasted image 20231216152321.png]]

## 模型评估与选择
### 训练误差与测试误差
训练误差 在训练集上
![[Pasted image 20231216152644.png]]
测试误差 在测试集上
![[Pasted image 20231216152733.png]]

误差率与准确率
下图给的是测试集上的误差率与准确率
![[Pasted image 20231216152839.png]]
### 过拟合
过拟合（over-fitting):学习所得模型包含参数过多，出现对已知数据预测很好，但对位置数据预测很差的现象。本质是一味提高对训练数据的预测能力，导致所选模型的复杂度比真模型更高。
### 正则化
正则化是结构风向最小化策略的实现。

正则化项一般是衡量模型复杂度函数，复杂度越大，正则化项越大。

![[Pasted image 20231216153953.png]]

两种模型：
![[Pasted image 20231216154215.png]]

奥卡姆剃刀原理：选择可能模型中，能够很好解释已知数据并且十分简单的模型。


### 交叉验证
训练集 验证集 测试集
![[Pasted image 20231216154532.png]]
重复使用数据来解决数据不充足情况：
简单交叉验证：
![[Pasted image 20231216154712.png]]
S折交叉验证：
![[Pasted image 20231216154828.png]]
留一交叉验证
S = N的S折交叉验证
## 泛化能力
泛化能力（generalization ability）是指有该学习方法得到的模型对未知数据的预测能力。
![[Pasted image 20231216163807.png]]
### 泛化误差上界
![[Pasted image 20231216163932.png]]

## 生成模型与判别模型


![[Pasted image 20231216164551.png]]



![[Pasted image 20231216164644.png]]




# 感知机
感知机（perception）是二分类的线性分类模型。
感知机对应于输入空间中将实例分为正负两类的分离超平面。

模型：
![[Pasted image 20231216170016.png]]
## 梯度下降法

算法过程
![[Pasted image 20231219193405.png]]

## 感知机原始形式

**问题定义**
**注意使用的是误分类的点**
![[Pasted image 20231219193839.png]]


随机梯度下降法
批量梯度下降法和 随机梯度下降算法
![[Pasted image 20231219194153.png]]

**算法总结：**
（采用随机梯度下降法）
![[Pasted image 20231219194326.png]]
注意：得到的分离超平面是不唯一的（和选取的wb初值和误分类点相关）



## 感知机对偶形式

对偶形式理解：[(7 封私信 / 80 条消息) 如何理解感知机学习算法的对偶形式？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/26526858)
学习的是ni

![[Pasted image 20231219195017.png]]

问题：
![[Pasted image 20231219195341.png]]
## 算法的收敛性

Novikoff定理 ！！！
# K近邻法

## 模型
三要素：距离度量 k值选择 分类决策规则

### 距离度量
Lp距离：n维的距离
闵可夫斯基距离(Minkowski Distance)
![[Pasted image 20231219212721.png]]
曼哈顿距离和切比雪夫距离
![[Pasted image 20231219212831.png]]
### k值选择
不理解 估计误差和近似误差
![[Pasted image 20231219213655.png]]
### 分类决策规则
![[Pasted image 20231219213949.png]]





## kd树
### 定义
![[Pasted image 20231219215336.png]]


例如 k=2的kd树
![[Pasted image 20231219215351.png]]
k=3的kd树
![[Pasted image 20231219215426.png]]


所谓的kd树就是用k-1维的量去划分k维空间。
### 构造kd树
如何选取xi？
计算每一个xi的方差，选取方差大的（说明xi的区分度大）
![[Pasted image 20231219215624.png]]
### 搜索kd树
**最近邻搜索**
![[Pasted image 20231221194141.png]]

感觉是正确的，但是不太理解，很奇怪






## 误差率
[3.1 k近邻法——简介_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1No4y1o7ac?p=18&spm_id_from=pageDriver&vd_source=87d1927df784895949e82f4c4367a812)

# 朴素贝叶斯
贝叶斯思维
![[Pasted image 20231221195511.png]]
复习：条件概率
![[Pasted image 20231221195955.png]]

贝叶斯定理：逆概率思维
全概率公式


贝叶斯分类
![[Pasted image 20231221200654.png]]



 
![[Pasted image 20231221200947.png]]

## 基本方法
![[Pasted image 20231221201632.png]]
求联合概率分布

![[Pasted image 20231221201823.png]]

后验概率最大化准则
![[Pasted image 20231221202923.png]]
对比损失最小化，转化为损失最小化
![[Pasted image 20231221203257.png]]

## 极大似然估计  
通过样本估计总量
![[Pasted image 20231221203828.png]]


极大似然估计原理：
使概论最大化
![[Pasted image 20231221204323.png]]


实现参数估计：
例1：

![[Pasted image 20231221211632.png]]
![[Pasted image 20231221211643.png]]


**极大似然估计的套路**

![[Pasted image 20231221212811.png]]

### 算法
[4.5 朴素贝叶斯法：算法_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1No4y1o7ac?p=28&spm_id_from=pageDriver&vd_source=87d1927df784895949e82f4c4367a812)
![[Pasted image 20231221214453.png]]


## 贝叶斯估计


模型 
![[Pasted image 20231221214910.png]]


### 原理


# 决策树
## 模型
### 决策树定义
归纳的思想（归纳就是从个别到一般的推理过程，而演绎则是从一般到个别的推理过程。）

![[Pasted image 20231223184924.png]]


### if-then规则
决策树就是if-then规则集合。
- 每个实例对应唯一的路径
- 分叉互斥且完备

### 构建决策树
最优特征，正确分类
![[Pasted image 20231223185440.png]]
借助于熵分类
### 决策树与条件概率分布
决策树可以表示为给定条件下类的条件概率分布。

定义在特征空间上的一个划分
![[Pasted image 20231223190019.png]]

如何确定某个单元是正类还是负类
![[Pasted image 20231223190002.png]]
立体图
![[Pasted image 20231223190225.png]]

根据条件概率确定决策树
![[Pasted image 20231223190406.png]]
### 决策树的学习
目的：根据训练集构造决策树
![[Pasted image 20231223190507.png]]
总结：
![[Pasted image 20231223190815.png]]

## 特征选择
### 信息增益与熵

信息论的知识
**等概率分布熵最大**
![[Pasted image 20231223191335.png]]
条件熵
![[Pasted image 20231223191920.png]]
计算信息增益：
![[Pasted image 20231223191951.png]]



## 决策树的生成
### ID3算法
过程
![[Pasted image 20231223193930.png]]
### C4.5算法
![[Pasted image 20231223195215.png]]
+ 利用信息增益比
+ 可以操作连续变量

## 决策树的剪枝
![[Pasted image 20231223200834.png]]

剪枝：处理决策树的过拟合问题
![[Pasted image 20231223204539.png]]

可以看出衡量是否剪枝是通过 判断泛化能力, 如何判断泛化能力？
### 预剪枝
![[Pasted image 20231223205017.png]]
使用测试集上的误差率

### 后剪枝
![[Pasted image 20231223210510.png]]
#### REP
自下而上的进行剪枝
降低错误剪枝
![[Pasted image 20231223213452.png]]
特点：**测试集影响很大**
#### PEP
自上而下剪枝
悲观错误剪枝
步骤
![[Pasted image 20231223213641.png]]
例题1：
![[Pasted image 20231223214622.png]]
例题2：
![[Pasted image 20231223214650.png]]
#### MEP
最小错误剪枝 
![[Pasted image 20231223214908.png]]
原理：
![[Pasted image 20231224183538.png]]
算法步骤
![[Pasted image 20231224183605.png]]
实例：
![[Pasted image 20231224184004.png]]
#### EBP 
基于错误剪枝
![[Pasted image 20231224184059.png]]

#### CCP
代价复杂度剪枝
![[Pasted image 20231224184415.png]]
**损失函数**
![[Pasted image 20231224185322.png]]
算法步骤
![[Pasted image 20231224185441.png]]

### CART算法


![[Pasted image 20231224185610.png]]

树的标准：二叉树
![[Pasted image 20231224190001.png]]
#### 基尼指数
使用基尼指数度量特征

![[Pasted image 20231224190353.png]]
在特征条件下，基尼指数
![[Pasted image 20231224190900.png]]
![[Pasted image 20231224191205.png]]
上述两个例子，甜度基尼系数小，区分更加明显，选择甜度作为特征向量。

#### 算法
**分类树算法**
![[Pasted image 20231224191447.png]]


**回归树算法**

![[Pasted image 20231224194615.png]]
如何选择切分变量和切分点
![[Pasted image 20231224195012.png]]
算法步骤
![[Pasted image 20231224195043.png]]
#### 剪枝
基于代价复杂度剪枝


